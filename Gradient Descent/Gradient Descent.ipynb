{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Logistic Regression\n",
    "\n",
    "$$ z = w^Tx+b $$\n",
    "$$ \\widehat{y} = a = \\sigma(z) => \\frac{1}{1+e^{-z}} $$\n",
    "Given $${(x^1, y^1), (x^2, y^2) ... , (x^m, y^m)}, \\space needed \\space a^i == y^i$$\n",
    "\n",
    "while trying to minimize the loss(error) if we use mean of squared error, $$ L(a, y) = \\frac{1}{2}|a - y|^2$$\n",
    "\n",
    "while trying to learn the parameters the optimization problem will be non-convex and eventually we might end up with multiple local optima. Thats the reason the loss function for Logistic Regression is,\n",
    "$$ associated \\space loss \\space L(a; y) = y\\space log(a) + (1-y)\\space log(a)$$\n",
    "\n",
    "And associated cost is $$ J(w, b) = \\frac{1}{m}\\sum_{i=1}^mL(a^i, y^i)$$\n",
    "$$ J(w, b) = \\frac{1}{m}\\sum_{i=1}^my^i\\space log(a^i) + (1-y^i)\\space log(a^i)$$\n",
    "\n",
    "Goal of the Logistic Regression is to reduce the loss by modifying the parameters **w** and **b**. For which Gradient Descent algorithm was developed.\n",
    "\n",
    "###### Gradient Descent\n",
    "\n",
    "**Gradient descent**, also known as **steepest descent**, is an optimization algorithm for finding the local minimum of a function. To find a local minimum, the function \"steps\" in the direction of the negative of the gradient.\n",
    "\n",
    "Here **gradient** is nothing but the slope of the loss function with respect to a single variable.\n",
    "\n",
    "In our case we are supposed to modify the values of **w** and **b**, which means we need to compute the slope of J(w, b) with respect to **w** and **b**. Which inturn is nothing but to compute the derivative of the Loss equation with respect to a variable.\n",
    "\n",
    "Once we have the partial derivate of J(w, b) we can update **w** and **b** using below update rules\n",
    "\n",
    "$$w:=w\\space -\\space \\alpha \\space \\frac{\\partial J(w, b)}{\\partial w}$$\n",
    "\n",
    "$$b:=b\\space -\\space \\alpha \\space \\frac{\\partial J(w, b)}{\\partial b}$$\n",
    "\n",
    "Let's assume we had two features,\n",
    "\n",
    "$$z = w_1\\space x_1 + w_2\\space x_2 + b $$\n",
    "\n",
    "$$\\widehat{y} = a = \\sigma(z) $$\n",
    "\n",
    "$$L(a; y) = -(y\\space log(a) + (1-y)\\space log(a))$$\n",
    "\n",
    "<img src=\"image.png\">\n",
    "\n",
    "Above sequence of steps define how to compute the loss associated with a single data point. All we need to do is to go backewards with this error to update the values of **w** and **b**.\n",
    "\n",
    "We can do this by computing the derivates of the Loss equation -\n",
    "\n",
    "$$\\frac{\\partial L(a,y)}{\\partial a} = -\\frac{y}{a}+\\frac{1-y}{1-a}$$\n",
    "\n",
    "$$\\frac{\\partial L(a,y)}{\\partial z} = a - y$$\n",
    "\n",
    "Using the chain rule we can express the derivative of loss function with respect to z as below,\n",
    "$$\\frac{\\partial L(a,y)}{\\partial y} = \\frac{\\partial L(a;y)}{\\partial a}.\\frac{\\partial a}{\\partial z}$$\n",
    "\n",
    "and that implies\n",
    "\n",
    "$$\\frac{\\partial a}{\\partial z} => \\space a(1-a)$$\n",
    "\n",
    "In the above case with two variables we need to compute the derivative of loss function with respect to each feature's coeffecient separately as follows,\n",
    "\n",
    "$$\\text{d}w_1 = \\frac{\\partial L}{\\partial w1} = x_1.\\frac{\\partial L(a,y)}{\\partial z}$$\n",
    "\n",
    "$$\\text{d}w_2 = \\frac{\\partial L}{\\partial w2} = x_2.\\frac{\\partial L(a,y)}{\\partial z}$$\n",
    "\n",
    "$$\\text{d}b  = \\frac{\\partial L}{\\partial b} = \\frac{\\partial L(a,y)}{\\partial z}$$\n",
    "\n",
    "Using the above derivatives we now update the values of the respective coeffecients and intercepts as follows\n",
    "\n",
    "$$w_1 := w_1 - \\text{d}w_1$$\n",
    "\n",
    "$$w_2 := w_2 - \\text{d}w_2$$\n",
    "\n",
    "$$b := b - \\text{d}b$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
