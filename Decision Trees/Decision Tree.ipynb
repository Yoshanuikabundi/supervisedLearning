{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Decision Trees\n",
    "Have you ever played a game called Twenty Questions? If not, the game works like this, One person thinks of some object and players try to guess the object. Players are allowed to ask 20 questions and the person is allowed to give only yes or no answers. In this game, the people asking the questions are successively splitting the set of objects they can deduce. A decision tree works just like the game Twenty Questions. You give it a bunch of data and it generates answers to the game.\n",
    "\n",
    "The Decision tree is one of the most commonly used classification techniques, they are commonly learned by recursively splitting the set of training instances into subsets based on the feature values of the given data points.\n",
    "\n",
    "**Example**\n",
    "<img src = \"images/decision_tree.png\">\n",
    "Above image of a decision tree has <i>decision blocks </i>(rectangles) and <i>terminating blocks</i> (ovals) where some conclusion will be obtained.  The arrows coming out of decision blocks are known as brnaches may lead to other decision branches or terminating blocks. \n",
    "\n",
    "Above example is build considering a hypothetical scenario of classifying emails. We first see the sender's domain in an email if it matches with myEmployer.com then we classify the emails as \"Email to read when bored.\" If not we check to see if the body has word **hockey**, if it has that word we classify it as \"Email from friends; read immediately\" else we mark it as \"Spam; don't read.\"\n",
    "\n",
    "The decision tree does a great job of distilling data into knowledge. With this, you can take a set of unfamiliar data and extract a set of rules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Training decision trees\n",
    "Here we will be creating an algorithm called Iterative Dichotomiser (ID3). Assume that you have to classify animals as cats or dogs. Unfortunately, you cannot observe the animals directly and must use only a few attributes of the animals to make your decision. For each animal, you are told whether or not it likes to play fetch, whether or not it is frequently grumpy, and its favorite of three types of food.\n",
    "\n",
    "To classify new animal, the decision tree will examine an explanatory variable at each decision node. The edge it follows to the next node will depend on the outcome of that node. For example, the first node might ask whether or not the animal likes to play fetch. If yes, we will follow the edge to the left child node, if not, we will follow the edge to the right child node. And we try to pose many such questions to narrow down our answer and eventually an edge will connect to a leaf node that indicates whether the animal is a cat or a dog.\n",
    "\n",
    "The following fourteen records make our training data:\n",
    "<img src = \"images/data.png\">\n",
    "\n",
    "From this data we can see that cats are generally grumpier than the dogs. Most dogs play fetch and most cats refuse. Dogs prefer dog food and bacon, whereas cats like cat food and bacon.\n",
    "\n",
    "From this table, we can manually construct classification rules. For example, an animal that is grumpy and likes cat food must be a cat, while an animal that plays fetch and likes bacon must be a dog. Constructing these classification rules by manually might be cumbersome even for a small data set. Instead, we will learn these rules by creating a decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Selecting questions\n",
    "One should always avoid creating questions that can separate only single record, such questions might infrequently classify a record but will not reduce the uncertainity. Questions that reduce the uncertainity about the classification the most are the best. Entropy is considered as the unit of measure used for quantifying the amount of uncertainity that a question can reduce.\n",
    "\n",
    "Measured in bits, entropy quantifies the amount of uncertainty in a variable. Entropy is given by the following equation,\n",
    "$$H(X) = - \\sum_{i=1}^{n}P(x_i)\\log_bP(x_i)$$\n",
    "\n",
    "where,\n",
    "\n",
    "\n",
    "    n - number of outcomes\n",
    "    P(x_i) - probability of the outcome i\n",
    "    b - base and it could be {2, 10, e} \n",
    "\n",
    "Because the log of a number less than one will be negative, the entire sum is negated to return a positive value.\n",
    "\n",
    "For example, a single toss of a fair coin has only two outcomes, heads and tails. The probability that the coin will land on heads is 0.5, and the probability that it will land on tails is 0.5. The entropy of the coin toss is equal to,\n",
    "\n",
    "$$H(X) = -(0.5\\log_2{0.5} + 0.5\\log_2{0.5} ) = 1.0$$\n",
    "\n",
    "Only one bit is required to represent the two equally probable outcomes, heads and tails. Two tosses of a fair coin can result in four possible outcomes. The probability of each outcome is 0.25. The entropy of two tosses is equal to the following,\n",
    "\n",
    "$$H(X) = -(0.25\\log_2{0.25}+0.25\\log_2{0.25}+0.25\\log_2{0.25}+0.25\\log_2{0.25}) = 2.0$$\n",
    "\n",
    "If the coin has the same face on both sides, the variable representing its outcome has 0 bits of entropy, as we are always certain of the outcome and the variable will never represent new information.\n",
    "\n",
    "Entropy can also be represented as a fraction of a bit. For example, if we consider an unfair coin that has probability of 0.8 to land on heads, and the probability that it will land on tails is 0.2. The entropy of a single toss of this coin is equal to,\n",
    "$$H(X) = -(0.8\\log_2{0.8} + 0.2\\log_2{0.2}) = 0.72192$$\n",
    "\n",
    "The outcome of a single toss of an unfair coin can have a fraction of one bit of entropy. There might be two possible outcomes of the toss, but as one outcome is more frequent we cannot be uncertain about the outcome.\n",
    "\n",
    "Going back to our example of classifying cats and dogs. If an equal number of dogs and cats comprise our data and we do not know anything else about the animal, the entropy of the decision is equal to one. Our data, however, contains six dogs and eight cats. If we do not know anything else about the unknown animal, the entropy of the decision is,\n",
    "\n",
    "$$H(X) = -\\left(\\frac{6}{14}\\log_2{\\frac{6}{14}} + \\frac{8}{14}\\log_2{\\frac{8}{14}}\\right) = 0.98522$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since cats are more common, we are less uncertain about the outcome, we have find a variable that reduces the entropy the most. We can make use of the <i>plays fetch</i> variable and divide the data into two subsets. \n",
    "<img src=\"images/ent1.png\">\n",
    "\n",
    "The top box in the diagram is the root node, it contains all of our training instances and specifies the variable that will be tested. At the root node we have not eliminated any instances from the training set and the entropy is equal to approximately 0.985. Based on the <i>plays fetch</i> variable's value. the data is divided into subsets. You can also see the newly calculated entropies for the subsets in the diagram.\n",
    "\n",
    "If we choose to use the <i>is grumpy</i> or <i> favorite food</i> variables then the tree would like as follows,\n",
    "<img src=\"images/ent2.png\">\n",
    "\n",
    "<img src=\"images/ent3.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Information Gain\n",
    "Testing for the animals that prefer cat food resulted in one subset with six cats, zero dogs, and 0 bits of entropy and another subset with two cats, six dogs, and 0.811 bits of entropy. How can we measure which of these tests reduced our uncertainty about the classification the most?\n",
    "\n",
    "Averaging the entropies of the subsets may be a good measure of the reduction in entropy. In the above example, the subsets produced by the cat food test have the lowest average entropy. Intuitively, this test seems to be effective, as we can use it to classify almost half of the training instances. However, selecting the test that produces the subsets with the lowest average entropy can produce only a suboptimal tree.\n",
    "\n",
    "\n",
    "For example, if we have a test that produces one subset with two dogs and no cats and another subset with four dogs and eight cats. The entropy of the first subset is equal to,\n",
    "$$H(X) = -\\left(\\frac{2}{2}\\log_2{\\frac{2}{2}}\\right) = 0$$\n",
    "\n",
    "the entropy of second subset is equal to,\n",
    "\n",
    "$$H(X) = -\\left(\\frac{4}{12}\\log_2{\\frac{4}{12}} + \\frac{4}{12}\\log_2{\\frac{4}{12}}\\right) = 0.9182$$\n",
    "\n",
    "The average of these subset entropies is only 0.459, but the second subset has almost one bit of entropy. This is analogous to asking specific questions early in Twenty Questions. We could get lucky and win within the first few attempts, but it is more likely that we will squander our questions without eliminating many possibilities.\n",
    "\n",
    "Hence people came up with a metric called as **Information Gain** to measure reduction in entropy. It is the difference between the entropy of the parent node, H(T), and the weighted average of the children node entropies.\n",
    "\n",
    "T is the set of instances, and a is the variable used to test in that node.\n",
    "\n",
    "$$x_a \\in vals(a) , \\space value\\space of \\space attribute \\space'a' \\space for \\space a \\space record \\space 'x'$$\n",
    "\n",
    "$$\\left\\{x \\in T | x_a = v \\right\\}$$\n",
    "is the number of instances for which attribute 'a' is equal to the value 'v'\n",
    "\n",
    "$$H({x \\in T | x_a = v})$$\n",
    "\n",
    "Above equation gives us the entropy of the subset in which value of variable 'a' is 'v'\n",
    "\n",
    "Now Information gain is ,\n",
    "\n",
    "$$IG(T, a) = H(T) - \\sum_{v\\in vals(a)} \\frac{|\\{x \\in T | x_a = v\\}|}{|T|} H({x \\in T | x_a = v})$$\n",
    "\n",
    "<img src = \"images/ig.png\">\n",
    "\n",
    "The above table shows shows the Information gain for all the tests and we can see that the cat food test is still the better among all with maximum Information gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try to add another node to the tree, right subset produced by cat food test contains only cats. where as left subsets has both cats and dogs, we will add a test to this node to to reduce the uncertainity even further. \n",
    "\n",
    "<img src = \"images/ig2.png\">\n",
    "\n",
    "As per above table, all of the tests produce subsets with 0 bits of entropy, but the is grumpy and plays fetch tests produce the greatest information gain. we usually breaks ties by selecting one of the best tests arbitrarily. We will select the is grumpy test, which splits its parent's eight instances into a leaf node containing four dogs and a node containing two cats and two dogs.\n",
    "\n",
    "Now our tree should be looking as below,\n",
    "<img src=\"images/ent4.png\">\n",
    "\n",
    "We will now select another variable to test the child node's four instances\n",
    "<img src=\"images/ig3.png\">\n",
    "All the tests produce equal information gain so, we will select the test randomly. Finally two more variables will be left out which will produce the same subsets and create a leaf node containing one dog and a another node containing two cats. Below is the final tree for our data.\n",
    "\n",
    "<img src=\"images/tree.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Gini impurity\n",
    "In the above section we created nodes based on the preference of maximum information gain. Another common approach for learning decision trees is **Gini impurity**, which measures the proportion of the classes in a set. \n",
    "\n",
    "$$Gini(t) = 1 - \\sum_{i-1}^{j}P(i\\space|\\space t)^2$$\n",
    "\n",
    "    j - number of classes\n",
    "    t - number of instances in the subset of a node\n",
    "    P(i | t) - probability of selecting an element of class i from subset\n",
    "\n",
    "Gini impurity is zero when all of the elements of the set are the same class, as the probability of selecting an element of that class is equal to one.\n",
    "\n",
    "Like entropy, Gini impurity is greatest when each class has an equal probability of being selected. The maximum value of Gini impurity depends on the number of possible classes, which is possible only during,\n",
    "\n",
    "$$Gini_{max} = 1 - \\frac{1}{n}$$\n",
    "\n",
    "Our problem has two classes, so the maximum value of the Gini impurity measure will be equal to 1/2."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Applying Decision Tree classifier for Internet Advertisements Dataset\n",
    "# http://archive.ics.uci.edu/ml/datasets/Internet+Advertisements\n",
    "\"\"\"\n",
    "The proportions of the classes are skewed, decision trees can produce biased trees from data with unbalanced class \n",
    "proportions; we will evaluate a model on the unaltered data set before deciding if it is worth balancing the \n",
    "training data by over or under sampling.\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "Feature Variables:\n",
    "dimensions of the image\n",
    "words from the containing page's URL\n",
    "words from the image's URL\n",
    "the image's alt text\n",
    "the image's anchor text\n",
    "a window of words surrounding the image tag\n",
    "Target:\n",
    "Image's class\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependences\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.grid_search import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading data\n",
    "df = pd.read_csv(\"ad.data\", header = None, low_memory = False)\n",
    "df.head()\n",
    "explanatory_variable_columns = set(df.columns.values)\n",
    "response_variable_column = df[len(df.columns.values)-1]\n",
    "# The last column describes the targets\n",
    "explanatory_variable_columns.remove(len(df.columns.values)-1)\n",
    "y = [1 if e == 'ad.' else 0 for e in response_variable_column]\n",
    "X = df[list(explanatory_variable_columns)]\n",
    "X.head()\n",
    "y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3798: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  method=method)\n"
     ]
    }
   ],
   "source": [
    "# We encoded the advertisements as the positive class and the content as the negative class. \n",
    "# More than 25% of the data points are having missing values and these missing values are marked by whitespace and \n",
    "# a question mark. We replaced them with -1 but imputing them would make more sense.\n",
    "X.replace(to_replace = ' *\\?', value = -1, regex = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 10)\n",
    "# X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# created a pipeline and criteria is set to entropy.\n",
    "pipeline = Pipeline([('clf', DecisionTreeClassifier(criterion='entropy'))])\n",
    "# specifying hyper parameters for GridSearch CV\n",
    "parameters = {\n",
    "'clf__max_depth': (150, 155, 160),\n",
    "'clf__min_samples_split': (2, 3),\n",
    "'clf__min_samples_leaf': (2, 3)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our metric for this task won't be accuracy, we will be monitoring the model based on its F1-score. Hence we set GridSearchCV to maximize the F1-score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  36 out of  36 | elapsed:   18.7s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('clf', DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best'))]),\n",
       "       fit_params={}, iid=True, n_jobs=-1,\n",
       "       param_grid={'clf__max_depth': (150, 155, 160), 'clf__min_samples_split': (2, 3), 'clf__min_samples_leaf': (2, 3)},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring='f1', verbose=1)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1, scoring='f1')\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: 0.870\n",
      "Best parameters set:\n",
      "\tclf__max_depth: 160\n",
      "\tclf__min_samples_leaf: 3\n",
      "\tclf__min_samples_split: 2\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.97      0.99      0.98       702\n",
      "          1       0.92      0.82      0.87       118\n",
      "\n",
      "avg / total       0.96      0.96      0.96       820\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print ('Best score: %0.3f' % grid_search.best_score_)\n",
    "print ('Best parameters set:')\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print ('\\t%s: %r' % (param_name, best_parameters[param_name]))\n",
    "print()\n",
    "predictions = grid_search.predict(X_test)\n",
    "print (classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
